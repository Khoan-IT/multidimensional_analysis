---
title: |
  | **Assignment:**
  | Classification of tumors using PCA, Clustering analysis, and ANOVA
date: "06/06/2024"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    keep_tex: no
    toc: no
    # toc_depth: 3
  html_document:
    # toc: no
    toc_depth: '3'
    df_print: paged
jelcodes: A00, B11, C22
fontsize: 12pt
margin: 2cm
linkcolor: Mahogany
citecolor: Mahogany
spacing: double
bibliography: ref.bib 
csl: apa.csl
---

<!-- # ```{r, out.width = "70%", fig.align='center', echo=FALSE, message=FALSE}
# knitr::include_graphics("logo.png")
# ``` -->


|  **ID** 	|       **Member**       	|    **Faculty**   	|
|:-------:	|:----------------------:	|:----------------: |
| 2370500 	| Le Duc Khoan   	        | Computer Science  |
| 2270522 	| Tran Trung Thai       	| Computer Science  |
| 1912046 	| Ngo Minh Hong Thai     	| Computer Science  |
| 2011290 	| Tran Hoang Nhat Huy    	| Computer Science  |

\newpage

\tableofcontents

\newpage

\listoffigures

\newpage

<!-- 

Trang 1:

title: "Assignment Report"

year: "2024"

program: "Department ..."

Group member: 

signature1: "person 1"

signature2: "person 2"

signature3: "person 3"

signature4: "person 4"

-->



<!-- 

Trang 2: List of figures

-->



<!-- 

Trang 3: List of tables

-->


<!-- 

Trang 4

-->

# **Introduction**
Medical analysis encompasses a wide array of methodologies and technologies aimed at understanding, diagnosing, treating, and preventing diseases and medical conditions. It plays a pivotal role in healthcare, utilizing various data sources such as clinical records, medical imaging, genetic information, and laboratory tests to derive insights that aid healthcare professionals in delivering optimal patient care. One significant application of medical analysis is in the field of oncology, particularly in the diagnosis and treatment of breast cancer. The Breast Cancer Wisconsin (Diagnostic) Data Set serves as a fundamental resource in this domain, offering a comprehensive collection of clinical and imaging data derived from fine needle aspirates (FNAs) of breast masses. This dataset, widely utilized in machine learning and data mining research, provides detailed features describing cell nuclei characteristics, allowing for the development and validation of predictive models to distinguish between benign and malignant breast tumors. Through medical analysis of datasets like the Breast Cancer Wisconsin (Diagnostic) Data Set, researchers and healthcare practitioners strive to improve early detection, prognosis, and treatment outcomes for breast cancer patients, ultimately advancing the field of oncology and enhancing patient care.

In the domain of medical analysis, the integration of advanced methodologies such as Principal Component Analysis (PCA), Hierarchical Clustering play pivotal roles in unraveling complex datasets like the Breast Cancer Wisconsin (Diagnostic) Data Set. This report embarks on an exploration of these analytical techniques and their application in unraveling crucial insights into breast cancer diagnosis and prognosis.

PCA serves as a cornerstone in dimensionality reduction, offering a means to distill the multidimensional complexity of medical datasets into a more manageable form. By transforming the original features into a smaller set of uncorrelated variables, PCA facilitates visualization, pattern recognition, and the identification of critical factors contributing to breast cancer diagnosis. In the context of the Breast Cancer Wisconsin (Diagnostic) Data Set, PCA holds promise in elucidating key biomarkers and phenotypic traits indicative of tumor malignancy.

Complementing PCA, Hierarchical Clustering offers a powerful method for uncovering inherent structures within the dataset. By grouping similar data points into clusters based on their similarity or dissimilarity, Hierarchical Clustering provides insights into the natural organization of the data, potentially revealing distinct subgroups of breast tumors or patient profiles. Through this clustering approach, patterns and correlations among variables can be discerned, contributing to a deeper understanding of breast cancer heterogeneity.

In addition to PCA and Hierarchical Clustering, the inclusion of ANOVA adds another layer of statistical rigor to the analysis. ANOVA enables the assessment of differences in means across multiple groups, allowing for the identification of significant variations in clinical or molecular features among different tumor types or patient cohorts. By incorporating ANOVA into the analytical framework, this report aims to elucidate the underlying factors driving breast cancer progression and treatment response, ultimately guiding clinical decision-making and personalized therapeutic interventions.

Through the integration of PCA, Hierarchical Clustering, and ANOVA, this report endeavors to unravel the complex landscape of breast cancer and pave the way for more precise diagnostic and therapeutic strategies. By leveraging these advanced analytical techniques, healthcare practitioners can gain deeper insights into the underlying mechanisms of breast cancer pathogenesis, ultimately leading to improved patient outcomes and enhanced precision in clinical practice.

<!--
http://doi.org/10.1109/TIM.2008.925015
https://doi.org/10.1111/j.1755-0238.2012.00182.x
https://doi.org/10.1016/j.jfca.2016.04.001
https://link.springer.com/article/10.1007/s00217-005-1169-5
-->

# **Methods**

## **Data description**

The Breast Cancer Wisconsin (Diagnostic) Data Set is a well-known dataset used for machine learning and data mining tasks, particularly in the context of medical research. It provides data for breast cancer diagnosis, where each sample is described by a set of features derived from digitized images of fine needle aspirates (FNAs) of breast masses. This dataset is widely used for developing and testing classification algorithms aimed at distinguishing between benign and malignant tumors.

The breast cancer datatset contains 569 samples of breast cancer cases. There is no missing attribute values in the dataset. Each sample is described by 32 variables, with 2 information variables: 

1. ID: Identifier for each patient. 

2. Diagnosis: The diagnosis of the breast mass (M = malignant, B = benign). 

and 30 numerical feature variables, resulting by computing 10 real-valued features in 3 metrics (mean, worst and se value): 

1. Radius: The radius of an individual nucleus is measured by averaging the length of radius line segments defined by centroid and individual mass points.

2. Texture: The texture of the cell nucleus is measured by finding the variance of the gray scale intensities in the component pixels

3. Perimeter: The total distance between the mass points constitutes the nuclear perimeter.

4. Area: Nuclear area is measured simply by counting the number of pixels on the interior of the mass and adding one-half of the pixels in the perimeter.

5. Smoothness: The smoothness of a nuclear contour is quantified by measuring the difference between the length of a radial line and the mean length of the lines surrounding it. This is similar to the curvature energy computation in the mass.

![Radial Lines used for Smoothness Computation](image/smoothness.png){width=30%}

6. Compactness: Compactness is a shape descriptor used to quantify how closely an object's shape resembles a circle. It combines perimeter and area to measure the compactness of cell nuclei using the formula perimeter^2/area. This dimensionless value is minimized by a perfect circle and increases with the irregularity of the boundary. However, this measure also increases for elongated cell nuclei, which do not necessarily indicate a higher likelihood of malignancy. Additionally, the feature tends to be biased upward for small cells due to decreased accuracy caused by digitization. To address the fact that no single shape measurement fully captures the concept of "irregularity," we employ several different shape features.

7. Concavity: In a further attempt to capture shape information, we measure the number and severity of concavities or indentations in a cell nucleus by drawing chords between non-adjacent contour points and assessing how much the actual boundary of the nucleus deviates inward from each chord. This parameter is significantly influenced by the length of these chords, with shorter chords better capturing small concavities. We chose to emphasize small indentations since larger shape irregularities are already addressed by other features.	

![Chords used to Compute Concavity](image/concavity.png){width=30%}

8. Concave points: This feature is similar to Concavity but measures only the number, rather than the magnitude, of contour concavities.

9. Symmetry: To measure symmetry, we first identify the major axis, which is the longest chord passing through the center of the cell. We then measure the difference in lengths between lines drawn perpendicular to the major axis and extending to the cell boundary in both directions. Special attention is given to cases where the major axis intersects the cell boundary due to a concavity.

![Segments used in Symmetry Computation](image/symmetry.png){width=30%}

10. Fractal dimension: The fractal dimension of a cell is approximated using the "coastline approximation".9 The perimeter of the nucleus is measured using increasingly larger 'rulers. As the ruler size increases, decreasing the precision of the measurement, the observed perimeter decreases. Plotting these to values on a log scale and measuring the downward slope gives (the negative of) an approximation to the fractal dimension. As with all the shape features, a higher value corresponds to a less regular contour and thus to a higher probability of malignancy.

![Sequence of Measurements for Computing Fractal Dimension](image/fractal_dimension.png){width=90%}

The mean value, extreme (largest) value (mean of the three largest values) and standard error (se) of each feature are computed for each image. The extreme values are the most intuitively useful for the problem at hand, since only a few malignant cells may occur in a given sample.

The second attribute in the dataset, which is Diagnosis variable, serves as the class identifier, with values in 2 distinct types: M is Malignant and B is Benign. The class distribution indicates that there are 357 instances for class B and 212 instances for class M.

<!--
- Origin and method of collecting data on the chemical composition of wines from three cultivars.
- Describe the structure of the data set, attributes and data columns.
-->


## **Data preprocessing using R**

```{r setup, include=FALSE} 

knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
library(ggplot2)
library(FactoMineR)
library(readxl)
library(httpgd)
library(factoextra)
library(tidyverse)
library(corrplot)
library(dbplyr)
library(readr)
library(cluster)
```

```{r, echo=FALSE, warning=FALSE}
# Import data
data <- read_csv("data.csv")

# Setting up the data
data <- data[, c(2:32)]
head(data)
data <- data %>% 
    rename("concave_points_se" = "concave points_se",
          "concave_points_worst" = "concave points_worst",
          "concave_points_mean" = "concave points_mean")
data <- as.data.frame(data)
```

<!--
- Describe data preprocessing steps such as normalization and handling missing data (if any).
- Explore data: statistical summaries, distribution plots, and correlations between components.
-->

### Data preparation

* Missing data

```{r}
# Check for missing values
missing_values <- colSums(is.na(data))

# Print columns with missing values
print(missing_values)

#Remove NA rows:
data <- na.omit(data)
```

* Outliers

<!--
```{r}
library(rstatix)

#identify_outliers(
#  data = data,
#  variable = c("radius_mean")
#)
tmp <- data
for (col in colnames(data)){
  if (col != "diagnosis"){
    outliers <- data %>%
      identify_outliers(variable = col, )
    tmp <- anti_join(tmp, outliers)
  }
}

data

tmp
```
-->

### Data exploration

* Boxplot 

```{r, include=FALSE, message=FALSE}
cap = c()

for (col in colnames(data)){
  cap[col] <- paste("Boxplot of Diagnosis vs.", colnames(data[col]))
  cap[col] <- gsub("_", "\\\\_", as.character(cap[col]))
}
```

```{r, tidy=FALSE, fig.align='center', fig.width=4, fig.height=4, fig.cap=cap[2:31]}
for (col in colnames(data)){
  if (col != 'diagnosis'){
    # boxplot(df.clean[, col], main=col)
    boxplot(data[, col] ~ data[, 'diagnosis'], xlab='diagnosis', ylab=col, main = NULL) 
  }
}
```

\newpage
* Histogram 

```{r, include=FALSE, message=FALSE}
cap = c()

for (col in colnames(data)){
  cap[col] <- paste("Histogram of", colnames(data[col]))
  cap[col] <- gsub("_", "\\\\_", as.character(cap[col]))
}
```

```{r, tidy=FALSE, fig.align='center', fig.width=4, fig.height=4, fig.cap=cap}
for (col in colnames(data)){
  if(col != 'diagnosis'){
    hist(data[, col], xlab=col, main = NULL)
  }
  else{
    barplot(table(data[, col]), xlab=col, ylab='frequency', main = NULL)
  }
}
```

<!--
* Scatter plot (pair)

```{r, tidy=FALSE, fig.align='center', fig.width=10, fig.height=10, fig.cap="Scatter plot of breast cancer dataset"}
  pairs(
    subset(data, select=-diagnosis),
    col = hcl.colors(3, "Temps")[data[, 'diagnosis']],
    cex=0.5,
    main = NULL
  )
```

* Correlation plot

```{r,  tidy=FALSE, fig.align='center', fig.width=5, fig.height=5, fig.cap="Correlation plot of breast cancer dataset"}
df.cor <- cor(data[-1], method = "pearson")
corrplot(
  df.cor, 
  is.corr=F, 
  type = "lower", 
  tl.cex = 0.5, 
  cl.cex = 0.5, 
  main = NULL
)
```
-->
\newpage

* Correlation matrix

```{r, tidy=FALSE, fig.align='center', fig.width=7, fig.height=7, fig.cap="Correlation plot of breast cancer dataset"}
cor_matrix <- cor(data[2:31])
corrplot(cor_matrix, method = "color", tl.cex = 0.5, 
         title = "Correlation Heatmap", mar = c(0,0,1,0))
```

## **Principal Component Analysis (PCA)**
<!--
- Explain the use of PCA to reduce data dimensionality.
- Visualize PCA results and explain principal components.
-->


```{r}
res.pca <- PCA(data, scale.unit=T, quali.sup="diagnosis", graph=F)
```

### Eigen values

```{r}
res.pca$eig

# Draw eigenvalues dist
barplot(
  res.pca$eig[, 1], 
  main = "Eigenvalues of Principal Components",
  xlab = "Principal Components",
  ylab = "Eigenvalues",
  cex.names = 0.4,  # Adjust size of the labels if necessary
  las = 2
)

# Draw eigenvalues dist
barplot(
  res.pca$eig[, 1][1:8], 
  main = "Eigenvalues of 8 Highest Principal Components",
  xlab = "Principal Components",
  ylab = "Eigenvalues",
  cex.names = 0.5,  # Adjust size of the labels if necessary
)
```

### Dimension Description

```{r}
dimdesc(res.pca)
```

### PCA Graph

```{r}
corrplot(
  res.pca$var$cos2, 
  is.corr = F,
  tl.cex = 0.5,
  cl.cex = 0.5,
  cl.pos = "r",
  cl.ratio = 0.5,  # Adjust ratio to add space to the legend
  cl.align.text = "l"  # Align the text to the left of the legend
)
```

```{r}
plot(
  res.pca,
  choix = c("ind"),
  hab = "diagnosis",
  invisible = c("var"),
  cex = 1,
  palette = c("red", "green"),
  autoLab = c("no"),
  title = "PCA Graph of Individuals",
  label = c("none")
)
```

```{r}
plot(
  res.pca,
  choix = c("var"),
  hab = "none",
  invisible = c("ind"),
  cex = 0.5,
  autoLab = c("no"),
  title = "PCA Graph of Variables"
)
```

## **Hierarchical Clustering (HC)**
<!--
- Describe and perform Hierarchical Clustering to determine relationships between chemical components.
-->

One technique for studying clusters in the analysis of data is hierarchical clustering. Data is grouped using hierarchical clustering, which builds a hierarchy of groups. There are two approaches to construct this hierarchy: divisive (top-down), in which all data points begin in a single cluster and divide as we proceed down the hierarchy, or agglomerative (bottom-up), in which each data point forms a cluster and pairs of clusters are merged as we travel up the hierarchy. In this work, the linkage criterion is Ward's linkage, and we employ agglomerative hierarchical clustering. The linking criterion defines appropriate distance between sets of observations to use.


```{r, tidy=FALSE, fig.align='center', fig.width=5, fig.height=5, fig.cap=c("Hierarchical Clustering plot", "Bar plot of Type vs. cluster percent")}
res.hcpc <- HCPC(res.pca, nb.clust=2, graph = FALSE)

fviz_dend(res.hcpc,
    cex = 0.7,                     
    labels_track_height = 0.8
)
  
df.hcpc <- res.hcpc$data.clust
  
plot(df.hcpc$diagnosis, df.hcpc$clust, xlab = 'diagnosis', ylab = '% cluster')

plot(df.hcpc$clust, df.hcpc$diagnosis, xlab = 'cluster', ylab = '% diagnosis')

plot.HCPC(res.hcpc,choice='map',draw.tree=FALSE,title='Factor map')
  
res.hcpc$desc.var$quanti
  
res.hcpc$desc.axes$quanti
```

#### Find best criteria for clustering
```{r}
clean_data <- data[2:31]
scaled_data <- scale(clean_data)
distance <- dist(scaled_data, method="euclidean")
```
We chose multiple methods to calculate the distance between the clusters such as average, single, complete and ward.

- Average: Measures the average (mean) distance between each observation in each cluster, weighted by the number of observations in each cluster

- Single: Measures the distance between the two closest points in each cluster

- Complete: Measures the distance between the two most distant points in each cluster

- Ward: Minimizes within cluster variance (sum of errors). Clusters are combined according to smallest between cluster distance.
```{r, tidy=FALSE, fig.align='center', fig.width=5, fig.height=5,}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(scaled_data, method = x)$ac
}

map_dbl(m, ac)
```
Here's a brief analysis of each method based on your results:

1. **Average (0.8632181)**:
  - The average linkage method (or UPGMA) considers the average distance between all pairs of objects from two clusters before merging them.
  
  - An agglomerative coefficient of 0.8632181 indicates a relatively good clustering structure but not the best among the methods provided. It means that the clusters are fairly well-defined but there might be some overlap.

2. **Single (0.8054129)**:
  - The single linkage method (or nearest neighbor) considers the minimum distance between objects from two clusters.

  - An agglomerative coefficient of 0.8054129 is the lowest among the four methods, suggesting that the clusters formed by this method are less well-defined. This method often results in long, "chain-like" clusters, which may not be ideal for all datasets.

3. **Complete (0.9006205)**:
  - The complete linkage method (or farthest neighbor) considers the maximum distance between objects from two clusters.
  
  - An agglomerative coefficient of 0.9006205 is quite high, indicating that this method produces well-defined clusters. This method tends to create compact clusters with small diameters, which can be beneficial for many applications.

4. **Ward (0.9739327)**:
  - Ward's method aims to minimize the total within-cluster variance. It is generally considered one of the most effective methods for hierarchical clustering.
  
  - An agglomerative coefficient of 0.9739327 is the highest among the methods, indicating that Ward's method has produced the most well-defined clusters for your data. This method is typically effective in producing clusters with small within-cluster variance.

Based on the agglomerative coefficients:

- **Ward's method** is the best choice for your data, as it has the highest coefficient (0.9739327), indicating the most well-defined clustering structure.
- **Complete linkage** also performs well with a high coefficient (0.9006205).
- **Average linkage** is decent but not as strong as complete or Ward's method.
- **Single linkage** has the lowest coefficient (0.8054129), suggesting it is the least effective method for your dataset in terms of forming well-defined clusters.

You might consider using Ward's method for clustering your data due to its superior performance in your analysis.
```{r, fig.align='center', fig.width=5, fig.height=5, fig.cap=c("Hierarchical Clustering with Single Linkage")}
hc <- agnes(scaled_data, method = "single")
sub_grp <- cutree(hc, k = 2)
fviz_cluster(list(data = scaled_data, cluster = sub_grp))
```
```{r, fig.align='center', fig.width=5, fig.height=5, fig.cap=c("Hierarchical Clustering with Ward Linkage")}
hc <- agnes(scaled_data, method = "ward")
sub_grp <- cutree(hc, k = 2)
fviz_cluster(list(data = scaled_data, cluster = sub_grp))
```
Accuracy with Ward Linkage
```{r, fig.align='center', fig.width=5, fig.height=5}
label <- factor(data$diagnosis)
label_vector <- match(label, unique(label))
comparison <- label_vector == sub_grp
acc <- table(comparison)
acc
```


## **Analysis of Variance (ANOVA)**

Analysis of variance (ANOVA) is a statistical technique that allows us to test the null hypothesis that the means of any three or more groups are the same against the alternative hypothesis that they are not equal using information from their samples. The fundamental idea behind the ANOVA technique is to compare the degree of variation within each sample to the degree of variance between samples in order to determine whether the population means differ from one another. It is assumed that the samples used in the ANOVA model come from normal populations with similar variances. An ANOVA with a single factor between subjects is used when there is only one factor and the analysis has more than two levels and multiple subjects in both experimental conditions.

# **Result**

## **PCA and HC classification**
<!--
- Classification results based on PCA and HC.
- Charts and tables illustrate the distinction between types of breast cancerr.
-->

### PCA interpretation

#### Eigenvalues

Eigenvalues, which are measurements of the variance explained by each main component, are important indicators in principal component analysis. Three principal components have been recognized in this investigation, and the variation they account for is shown by their associated eigenvalues. Notably, the first major component accounts for 38.85% of the variation in total, with the second and third accounting for 20.49% and 9.55% of the variance, respectively. Increased eigenvalues indicate a higher degree of information retention from the original dataset, highlighting the fact that PC1 contains the largest amount of variance derived from the wine data.

#### Contributions

Analysis of contributions reveals insights into links between continuous and categorical variable (Type) through the data table's display of the correlations between original variables and principal components. Their relevance is clarified by examining the contributions of the first three main components below.

The first principle component (PC1), representing 38.85% of the variance, demonstrates strong correlations with variables like Flavanoids, Phenols, Dilution, Proline, and Hue. Notably, Flavanoids, Phenols, Dilution, and Proline exhibit high positive correlations, signifying their significant contributions to the observed separation or variability in the data along this component. Moreover, the categorical variable 'Type' shows a substantial relationship with PC1, emphasizing its role in differentiating between wine types.

For the second principle component, it accounted for 20.49% of the variance, displays high correlations with variables such as Color, Alcohol, Proline, Ash, and Magnesium. Particularly, Color and Alcohol stand out with strong positive correlations, indicating their substantial contributions to the variation captured by PC2. Similarly, 'Type' showcases a robust relationship with PC2, underlining its importance in distinguishing between wine types along this component.

Finally, the third principle component represented 9.55% of the variance, exhibits notable correlations with variables like Ash, Alcalinity, Nonflavanoids, Dilution, and Flavanoids. Notably, Ash and Alcalinity display high positive correlations, indicating their significant contributions to the variability represented by PC3. Although the categorical variable 'Type' also demonstrates differentiation along PC3, its impact is not as pronounced as observed in PC1 and PC2.

### HC interpretation

As a result of the cluster analysis, specific quantitative factors that identified the distinctive qualities and contributions of every cluster were found to exist. There are 2 cluster:

- Cluster 1 contains about 90% of B tumors and 10% of M tumors.

- Cluster 2 contains about 10% of B tumors and 90% of M tumors.

First of all, Cluster 1, which is composed of more than 90% of benign tumors, has characteristics by having almost all values quite lower when compare to overall. Cluster 1 type are distinguished by characteristics that stand out: symmetry, fractal dimension, texture, smoothness, concavity, compactness, concave points, area and so on.

Opposite to cluster 2, which consists primarily of malignant tumors, has unique characteristics that are indicated by increased almost all values relative to the dataset's average values.

### Conclusion from PCA and HC

The PCA analysis identified key variables (chemical components) that significantly contribute to the differentiation of wine types. PC1, PC2, and PC3 collectively explain a substantial portion of the variance in the dataset. Variables like Flavanoids, Phenols, Dilution, Proline, Color, Alcohol, Ash, Alcalinity, etc., exhibit strong correlations with these principal components and are crucial in distinguishing between different wine types.

On the other hand, the hierarchical clustering analysis, along with associated quantitative variables and principal dimensions, provides a clear demarcation of wines into three distinct clusters (representing Types 1, 2, and 3). Each cluster exhibits unique characteristics based on the 13 attributes. These findings offer valuable insights into how different wine types/classes can be differentiated based on these chemical attributes, aiding in wine classification and potentially understanding the underlying characteristics that differentiate these types.

## **ANOVA test**
<!--
- Use ANOVA to determine differences in chemical composition between types of wine.
-->

The aim is to test the significance of various chemical components across different wine types. These tests in R will provide insights into which variables significantly differ between wine types, aiding in the classification of wines based on their chemical compositions.

### **Hypothesis 1:** Mean of Concave Points between benign and malignant tumors.

-   Null Hypothesis (H0): There is no significant difference in Mean of Concave Points between 2 tumor types.
-   Alternative Hypothesis (H1): There is a significant difference in Mean of Concave Points between 2 tumor types.

#### Test:

ANOVA for Mean of Concave Points among tumor types.

```{r}
# ANOVA for Mean of Concave Points among tumor types.
h1 <- lm(
  concave_points_mean ~ as.factor(diagnosis), 
  data = data
)
anova(h1)
```

#### Interpretation:

The ANOVA test indicates a highly significant difference in Mean of Concave Points among different tumor types (p = 2.2e-16 \< 0.001). This suggests that Mean of Concave Points vary strongly between benign and malignant tumor. Then **reject Hypothesis**.

#### Visualization:

```{r}
# Boxplot for Mean of Concave Points
boxplot(
  concave_points_mean ~ as.factor(diagnosis), 
  data = data, 
  xlab = "Tumor Type", 
  ylab = "Mean of Concave Pointst", 
  main = "Mean of Concave Points across Tumor Types"
)

plot.PCA(res.pca,
   choix = "ind",
   hab = "concave_points_mean",
   invisible = c("var"),
   cex = 1,
   autoLab = c("no"),
   title = "PCA Graph of Individuals With Respect to Means of Concave Points",
   label = c("none")
)
```

We can see that there is significant difference of mean of concave points between malignant and benign breast tumors. The mean of concave points of malignant breast tumors is much higher than that of benign ones as in the Box Plot. Recall the variable PCA plot, the mean of concave points is strongly positive correlated with Dim 1, which is also true in the PCA graph of individuals as we can see almost all malignant breast tumors are aligned to the right of Dim 1. Hence, they have higher mean of concave points. All mentions above prove that a high number of concave points usually the sign of breast cancer.

### **Hypothesis 2:** Mean of Concavity between benign and malignant tumors.

-   Null Hypothesis (H0): There is no significant difference in Mean of Concavity between 2 tumor types.
-   Alternative Hypothesis (H1): There is a significant difference in Mean of Concavity between 2 tumor types.

#### Test:

ANOVA for Mean of Concavity among tumor types.

```{r}
# ANOVA for Mean of Concavity among tumor types.
h2 <- lm(
  concavity_mean ~ as.factor(diagnosis), 
  data = data
)
anova(h2)
```

#### Interpretation:

The ANOVA test indicates a highly significant difference in Mean of Concavity among different tumor types (p = 2.2e-16 \< 0.001). This suggests that Mean of Concavity vary strongly between benign and malignant tumor. Then **reject Hypothesis**.

#### Visualization:

```{r}
# Boxplot for Mean of Concavity
boxplot(
  concavity_mean ~ as.factor(diagnosis), 
  data = data, 
  xlab = "Tumor Type", 
  ylab = "Mean of Concavity", 
  main = "Mean of Concavity across Tumor Types"
)

plot.PCA(res.pca,
   choix = "ind",
   hab = "concavity_mean",
   invisible = c("var"),
   cex = 1,
   autoLab = c("no"),
   title = "PCA Graph of Individuals With Respect to Means of Concavity",
   label = c("none")
)
```

Just like Mean of Concave Points, the mean of concavity is quite different between malignant and benign breast tumors. The mean of concavity of malignant breast tumors is much higher than that of benign ones as in the Box Plot. Recall the variable PCA plot, the mean of concavity is also strongly positive correlated with Dim 1 similar to the mean of concave points. We can see in the PCA graph of individuals that almost all malignant breast tumors are aligned to the right of Dim 1. Therefore, the bigger size of tumor, the higher chance of a person to have breast cancer.

### **Hypothesis 3:** Worst Concave Points between benign and malignant tumors.

-   Null Hypothesis (H0): There is no significant difference in Worst Concave Points between 2 tumor types.
-   Alternative Hypothesis (H1): There is a significant difference in Worst Concave Points between 2 tumor types.

#### Test:

ANOVA for Worst Concave Points among tumor types.

```{r}
# ANOVA for Worst Concave Points among tumor types.
h3 <- lm(
  concave_points_worst ~ as.factor(diagnosis), 
  data = data
)
anova(h3)
```

#### Interpretation:

The ANOVA test indicates a highly significant difference in Worst Concave Points among different tumor types (p = 2.2e-16 \< 0.001). This suggests that Mean of Concavity vary strongly between benign and malignant tumor. Then **reject Hypothesis**.

#### Visualization:

```{r}
# Boxplot for Worst Concave Points
boxplot(
  concave_points_worst ~ as.factor(diagnosis), 
  data = data, 
  xlab = "Tumor Type", 
  ylab = "Worst Concave Points", 
  main = "Worst Concave Points across Tumor Types"
)

plot.PCA(res.pca,
   choix = "ind",
   hab = "concavity_worst",
   invisible = c("var"),
   cex = 1,
   autoLab = c("no"),
   title = "PCA Graph of Individuals With Respect to Worst Concave Points",
   label = c("none")
)
```

Similar to 2 stats of concavity mentioned above, the Worst Concave Points is different between 2 type of breast tumors. The Worst Concave Points of malignant breast tumors is much higher than that of benign ones as in the Box Plot. Also, the Worst Concave Points is positively correlated with Dim 1 and just like the concavity stats above, almost all malignant breast tumors are aligned to the right of Dim 1. Therefore, the worse concavity of tumor, the higher chance of a person to have breast cancer.

### **Hypothesis 4:** Mean of Compactness between benign and malignant tumors.

-   Null Hypothesis (H0): There is no significant difference in Mean of Compactness between 2 tumor types.
-   Alternative Hypothesis (H1): There is a significant difference in Mean of Compactness between 2 tumor types.

#### Test:

ANOVA for Mean of Compactness among tumor types.

```{r}
# ANOVA for Mean of Compactness among tumor types.
h4 <- lm(
  compactness_mean ~ as.factor(diagnosis), 
  data = data
)
anova(h4)
```

#### Interpretation:

The ANOVA test indicates a highly significant difference in Mean of Compactness among different tumor types (p = 2.2e-16 \< 0.001). This suggests that Mean of Compactness vary strongly between benign and malignant tumor. Then **reject Hypothesis**.

#### Visualization:

```{r}
# Boxplot for Mean of Compactness
boxplot(
  compactness_mean ~ as.factor(diagnosis), 
  data = data, 
  xlab = "Tumor Type", 
  ylab = "Mean of Compactness", 
  main = "Mean of Compactness across Tumor Types"
)

plot.PCA(res.pca,
   choix = "ind",
   hab = "compactness_mean",
   invisible = c("var"),
   cex = 1,
   autoLab = c("no"),
   title = "PCA Graph of Individuals With Respect to Means of Compactness",
   label = c("none")
)
```

Here, Mean of Compactness is different between 2 type of breast tumors. The Mean of Compactness of malignant breast tumors is also higher than that of benign ones as in the Box Plot. And, the Mean of Compactness is positively correlated with Dim 1 and just like all the concavity stats above, almost all malignant breast tumors are aligned to the right of Dim 1.

### Other hypotheses on an variable between benign and malignant tumors.

Similar to the analysis above and the Dimension Description of the PCA on the dataset, we can find some other variables that are:

- Significantly different between benign and malignant tumors. 

- Positively correlated with Dim 1 of PCA variable graph. Means that higher value correspond to higher chance of breast cancer.

These variables are: 

- perimeter_worst 

- concavity_worst 

- radius_worst 

- perimeter_mean 

- area_worst 

- area_mean

We will perform ANOVA analysis and visualize the relations of these variable below with corresponding order: \#### Worst Perimeter

```{r}
h4.1 <- lm(
  perimeter_worst ~ as.factor(diagnosis), 
  data = data
)
anova(h4.1)

# Boxplot for Worst Perimeter
boxplot(
  perimeter_worst ~ as.factor(diagnosis), 
  data = data, 
  xlab = "Tumor Type", 
  ylab = "Worst Perimeter", 
  main = "Worst Perimeter across Tumor Types"
)

plot.PCA(res.pca,
   choix = "ind",
   hab = "perimeter_worst",
   invisible = c("var"),
   cex = 1,
   autoLab = c("no"),
   title = "PCA Graph of Individuals With Respect to Means of Worst Perimeter",
   label = c("none")
)
```

#### Worst Concavity

```{r}
h4.2 <- lm(
  concavity_worst ~ as.factor(diagnosis), 
  data = data
)
anova(h4.2)

# Boxplot for Worst Perimeter
boxplot(
  concavity_worst ~ as.factor(diagnosis), 
  data = data, 
  xlab = "Tumor Type", 
  ylab = "Worst Concavity", 
  main = "Worst Concavity across Tumor Types"
)

plot.PCA(res.pca,
   choix = "ind",
   hab = "concavity_worst",
   invisible = c("var"),
   cex = 1,
   autoLab = c("no"),
   title = "PCA Graph of Individuals With Respect to Means of Worst Concavity",
   label = c("none")
)
```

#### Worst Radius

```{r}
h4.3 <- lm(
  radius_worst ~ as.factor(diagnosis), 
  data = data
)
anova(h4.3)

# Boxplot for Worst Radius
boxplot(
  radius_worst ~ as.factor(diagnosis), 
  data = data, 
  xlab = "Tumor Type", 
  ylab = "Worst Radius", 
  main = "Worst Radius across Tumor Types"
)

plot.PCA(res.pca,
   choix = "ind",
   hab = "radius_worst",
   invisible = c("var"),
   cex = 1,
   autoLab = c("no"),
   title = "PCA Graph of Individuals With Respect to Means of Worst Radius",
   label = c("none")
)
```

#### Mean Perimeter

```{r}
h4.4 <- lm(
  perimeter_mean ~ as.factor(diagnosis), 
  data = data
)
anova(h4.4)

# Boxplot for Mean Perimeter
boxplot(
  perimeter_mean ~ as.factor(diagnosis), 
  data = data, 
  xlab = "Tumor Type", 
  ylab = "Mean Perimeter", 
  main = "Mean Perimeter across Tumor Types"
)

plot.PCA(res.pca,
   choix = "ind",
   hab = "perimeter_mean",
   invisible = c("var"),
   cex = 1,
   autoLab = c("no"),
   title = "PCA Graph of Individuals With Respect to Means of Mean Perimeter",
   label = c("none")
)
```

#### Worst Area

```{r}
h4.5 <- lm(
  area_worst ~ as.factor(diagnosis), 
  data = data
)
anova(h4.5)

# Boxplot for Worst Area
boxplot(
  area_worst ~ as.factor(diagnosis), 
  data = data, 
  xlab = "Tumor Type", 
  ylab = "Worst Area", 
  main = "Worst Area across Tumor Types"
)

plot.PCA(res.pca,
   choix = "ind",
   hab = "area_worst",
   invisible = c("var"),
   cex = 1,
   autoLab = c("no"),
   title = "PCA Graph of Individuals With Respect to Means of Worst Area",
   label = c("none")
)
```

#### Mean Area

```{r}
h4.6 <- lm(
  area_mean ~ as.factor(diagnosis), 
  data = data
)
anova(h4.6)

# Boxplot for Mean Area
boxplot(
  area_mean ~ as.factor(diagnosis), 
  data = data, 
  xlab = "Tumor Type", 
  ylab = "Mean Area", 
  main = "Mean Area across Tumor Types"
)

plot.PCA(res.pca,
   choix = "ind",
   hab = "area_mean",
   invisible = c("var"),
   cex = 1,
   autoLab = c("no"),
   title = "PCA Graph of Individuals With Respect to Means of Mean Area",
   label = c("none")
)
```

### **Hypothesis 5:** Mean Fractal dimension between benign and malignant tumors.

-   Null Hypothesis (H0): There is no significant difference in Mean Fractal dimension between 2 tumor types.
-   Alternative Hypothesis (H1): There is a significant difference in Mean Fractal dimension between 2 tumor types.

#### Test:

ANOVA for Mean Fractal dimension among tumor types.

```{r}
# ANOVA for Mean Fractal dimension among tumor types.
h5 <- lm(
  fractal_dimension_mean ~ as.factor(diagnosis), 
  data = data
)
anova(h5)
```

#### Interpretation:

Eventhough the high correlations with the second principal component
(Dim.2) suggest that Mean Fractal dimension significantly contribute to
the variability captured by Dim 2. However, the high p-value in the
ANOVA test indicates that the Mean Fractal dimension does not
significantly differ between the diagnosed groups (malignant and benign)
when considered alone. (p = 0.7599). Then **accept Hypothesis**.

#### Visualization:

```{r}
# Boxplot for Mean Fractal dimension
boxplot(
  fractal_dimension_mean~ as.factor(diagnosis), 
  data = data, 
  xlab = "Tumor Type", 
  ylab = "Mean Fractal dimension", 
  main = "Mean Fractal dimension across Tumor Types"
)

plot.PCA(res.pca,
   choix = "ind",
   hab = "fractal_dimension_mean",
   invisible = c("var"),
   cex = 1,
   autoLab = c("no"),
   title = "PCA Graph of Individuals With Respect to Mean Fractal dimension",
   label = c("none")
)
```

For this variables, the high correlations with the second principal component (Dim.2) does not indicate that this variable varies across different tumor types. It suggests that Mean Fractal dimension significantly contribute to the variability captured by Dim 2. As we can see in the box plot that the mean values of 2 types are no different at all. Therefore we cannot reject the null hypothesis.

### **Hypothesis 6:** All 3 Fractal dimension variables effect on benign and malignant tumors.

-   Null Hypothesis (H0): There is no association between the diagnosis of the patients and their combined fractal dimension values.
-   Alternative Hypothesis (H1): There is an association between the diagnosis of the patients and their combined fractal dimension values.

#### Test:

ANOVA for 3 Mean Fractal dimension variables among tumor types.

```{r}
# ANOVA for 3 Mean Fractal dimension variables among tumor types.
h6 <- lm(
  fractal_dimension_mean + fractal_dimension_se + fractal_dimension_worst ~ as.factor(diagnosis), 
  data = data
)
anova(h6)
```

#### Interpretation:

When we used the combination of the 3 variables of Fractal dimension, this leads to a small p-value (2.155e-08 \< 0.001), then there must be an association between the diagnosis of the patients and their combined fractal dimension values =\> **reject Hypothesis**.

### **Hypothesis 7:** Worst smoothness and symmetry effect on benign and malignant tumors.

-   Null Hypothesis (H0): There is no association between the diagnosis of the patients and their combined worst smoothness and worst smoothness.
-   Alternative Hypothesis (H1): There is an association between the diagnosis of the patients and their combined worst smoothness and worst smoothness.

#### Test:

ANOVA for worst smoothness and symmetry among tumor types.

```{r}
# ANOVA for worst smoothness and symmetry among tumor types.
h7 <- lm(
  smoothness_worst + symmetry_worst  ~ as.factor(diagnosis), 
  data = data
)
anova(h7)
```

#### Interpretation:

When we used the combination of worst smoothness and symmetric, this leads to a small p-value (2.2e-16 \< 0.001), then there must be an association between the diagnosis of the patients and their combined worst smoothness and symmetry =\> **reject Hypothesis**.

### **Hypothesis 8:** Mean of Smoothness and perimeter for benign and malignant tumors classification

#### Hypothesis

*   Null Hypothesis (H0): There is no significant relationship between mean of smoothness and perimeter with benign andmalignant tumors.

*   Alternative Hypothesis (H1): There is significant relationship between mean of smoothness and perimeter with benign and malignant tumors.

```{r}
    # Correlation tests or t-tests comparing mean of smoothness and perimeter across benign and malignant tumors.
    label <- factor(data$diagnosis)
    label <- match(label, unique(label))
    cor.test(data$perimeter_mean, data$smoothness_mean)

    oneway.test(data$perimeter_mean ~ as.factor(label))
    
    oneway.test(data$smoothness_mean ~ as.factor(label))
```
#### Interpretation

-  Pearson’s correlation and ANOVA both reveal significant relationships between mean of smoothness, perimeter
and tumors (p < 0.001) =\> **reject Hypothesis**. There’s a significant correlation between mean of smoothness and mean of perimeter.
Additionally, both variables significantly differ among tumors.

#### Visualization
```{r}
# Scatterplot for Alcalinity
plot(smoothness_mean ~ as.factor(diagnosis), data = data, xlab = "Tumors", ylab = "Mean of smoothness", main = "Mean of smoothness across Tumors")
plot(perimeter_mean ~ as.factor(diagnosis), data = data, xlab = "Tumors", ylab = "Mean of perimeter", main = "Mean of perimeter cross Tumors")


plot.PCA(res.pca,
  choix = "ind",
  hab = "smoothness_mean",
  invisible = c("var"),
  cex = 1,
  autoLab = c("no"),
  title = "PCA Graph of Individuals With Respect to mean of smoothness",
  label = c("none")
)

plot.PCA(res.pca,
  choix = "ind",
  hab = "perimeter_mean",
  invisible = c("var"),
  cex = 1,
  autoLab = c("no"),
  title = "PCA Graph of Individuals With Respect to mean of perimeter",
  label = c("none")
)
```

# **Conclusion and Discussion**

## **Summary of results**
<!--
- Summarize important findings from analysis and testing.
- Evaluate the level of success of the classification and the strengths/weaknesses of the method.
-->

### Keypoints from analysis and testing

The findings, particularly from PCA, delineate the pivotal role of specific features related to breast mass characteristics derived from digitized images of FNA of breast masses in diagnosing the existing of breast cancer.

<!--
PC1, chiefly influenced by Total Phenols, Flavonoid Phenols, Proanthocyanins, and OD280/OD315, predominantly reflects taste intensity. This implies that the strength of taste emerges as a defining characteristic in classifying wines. Conversely, PC2, representing alcohol and fermentation levels through variables such as Alcohol Content, Proline, Color Intensity, Ash Levels, and Magnesium Levels, underscores the importance of fermentation-related attributes in delineating wine types.
-->

With hierarchical clustering analysis, the study further emphasized the heterogeneity in the benign and malignant breast cancer patient groups, as evident by the clear division into two clusters (M or B tumors).

The conducted ANOVA tests further validate and emphasize the significance of these chemical components in distinguishing between the wine types.

<!--
Variables such as Flavanoids, Alcohol content, Color intensity, Proline content, and Proanthocyanins exhibit distinctive levels among the wine types, significantly contributing to the classification based on their chemical profiles. Despite Ash and Alcalinity offering limited insights into the differentiation of certain wine types individually, their collective data elucidates similarities among these types.
-->

### Statistical significance to the real-world problem

<!--
For the purpose of improving strength of the taste of wines: Winemakers can leverage this work to tailor the Flavanoid content in wines, influencing the taste, aroma, and overall sensory experience. Consumers, on the other hand, can use Flavanoid levels as a factor in selecting wines based on their preferred flavor profiles. Proline is associated with taste and quality. Wine manufacturers can use this information to craft wines with specific flavor profiles. Consumers looking for particular taste characteristics can consider Proline content in their wine choices. The significant differences in Proanthocyanins content among wine types indicate that this component is a key factor in differentiating between the wines. Understanding the role of Proanthocyanins is valuable for vintner aiming to create wines with specific taste profiles.

With the aim of levitating alcohol and fermentation levels: Winemakers can optimize Color intensity to create visually distinct wines. Consumers may consider color as a factor in wine selection, associating it with taste preferences or the overall appeal of the wine. Producers of wines can adjust Alcohol content to create wines with specific characteristics, catering to different preferences and occasions. Consumers may use this information to make choices based on desired Alcohol levels in wines. The significant relationships between Ash, Alcalinity, and wine types suggest that these components contribute to the classification of wines. Wine producers can use this information to adjust and optimize these elements to achieve desired characteristics in their wines. 

In essence, the statistical significance of these chemical components in wine types provides actionable insights for both wine merchants and consumers. It enables winemakers to craft wines with specific characteristics and empowers consumers to make informed choices based on their preferences, whether related to flavor, visual appeal, or health considerations.
-->

## **Comments and Limitations**
<!--
- Evaluate the reliability of the results and limitations of the study.
- Propose directions for further development and research.
-->

### Recommendations on data quality

In the pursuit of comprehensive data analysis, refining or improving the data table is crucial: firstly, ensuring consistency across columns, especially in categorical variables like 'diagnose,' to eliminate discrepancies and ensure accurate labeling. Next, leveraging feature engineering techniques to enhance analysis depth by creating new features or interaction terms based on domain knowledge. Employing cross-validation techniques becomes crucial for predictive modeling, assessing model performance comprehensively for robust findings. Thorough documentation of data preprocessing steps, including handling missing values, ensures transparency and reproducibility. Additionally, establishing a systematic process for ongoing dataset maintenance and updates guarantees sustained data quality and accuracy, especially for periodically updated datasets. These practices collectively fortify the integrity and reliability of the dataset and analyses conducted upon it.

### Data and classification method limitations

PCA is a powerful and flexible technique but it may lose some information and details when reducing data size. This can lead to simplification or distortion of the data and make it more difficult to identify outliers or anomalies. Another disadvantage of PCA is that it can be sensitive to scaling and outliers, which can affect the quality and stability of the results. 

For HC, this is a solid, flexible technique that is easy to understand and apply. However, it does not guarantee optimal results or the best possible clustering. The results can depend on the order in which data points are processed, making it difficult to reproduce or generalize them to other data sets with similar characteristics. It cannot handle categorical variables effectively because they cannot be converted to numeric values and thus will not produce meaningful clusters if used in hierarchical clustering algorithms.

Although applying PCA combined with hierarchical clustering and ANOVA makes it easier to visualize taxonomies and select the aspects of the data that contribute the most, this method cannot yield satisfactory classification results and high accuracy. Also, each classification method has its limitations. 

Thus, this study has limited capacity for precise interpretation, and require integratation of additional analytical techniques or refining existing ones might enhance the method's precision, allowing for more precise of breast cancer patients.

** Propose some method like K-means, DNN, ... **

\newpage

# **References**
<!--
- List of documents, books, articles used and referenced during the research process.
-->